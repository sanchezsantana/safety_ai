# Mining Safety Deep QA Agent

This repository contains the design and implementation plan for a **Deep QA Agent** focused on mining safety.  
It integrates **Qdrant (VDBs), Neo4j (GDB), predictive models (APIs), and a finetuned Gemma 3B LLM**, orchestrated through **LangChain**, with **Streamlit** as the frontend.

---

## ğŸ¯ Objectives

- Build a **modular, hierarchical QA agent** for mining safety.  
- Integrate multiple knowledge sources:  
  - **Ontology, technical docs, synthetic/real data (VDBs in Qdrant)**.  
  - **Structured knowledge (Neo4j AuraDB)**.  
  - **Predictive insights (external APIs)**.  
- Enable **robust reasoning** with fallback and reflection.  
- Provide **explainable and reproducible** outputs.  
- Deliver an interactive **Streamlit-based QA interface**.  

---

## ğŸ”¹ Architecture Overview

The QA system follows a **hierarchical multi-agent architecture** inspired by `deepagents`:

- **Master Agent (Gemma 3B finetuned)**  
  Coordinates reasoning and decides which sub-agents to use.

- **Sub-Agents**  
  - **VDB Agent** â†’ Semantic search with **Qdrant** (3 collections: ontology, technical docs, synthetic/real data).  
  - **GDB Agent** â†’ Queries structured knowledge via **Neo4j AuraDB (Cypher)**.  
  - **Predictive Agent** â†’ Calls external APIs for predictive models.  

---

## ğŸ”¹ Reasoning Loop

We extend beyond the classic **ReAct** with a **Plan â†’ Execute â†’ Reflect â†’ Improve** cycle:

1. **Plan** â†’ Decide which sub-agent(s) are required.  
2. **Execute** â†’ Query tools (Qdrant, Neo4j, Predictive API).  
3. **Reflect** â†’ Validate results, fallback if empty or inconsistent.  
4. **Improve** â†’ Adjust tool usage dynamically for robustness.  

This ensures **resilient, self-correcting reasoning**.

---

## ğŸ”¹ Flow Diagram

```mermaid
flowchart TD
    A[User Query] --> B[Master Agent - Gemma 3B FT]
    B -->|Semantic search| C[VDB Agent - Qdrant]
    B -->|Graph reasoning| D[GDB Agent - Neo4j]
    B -->|Predictions| E[Predictive Agent - API]
    C --> F[Reflect & Merge]
    D --> F[Reflect & Merge]
    E --> F[Reflect & Merge]
    F --> G[Final Answer to User via Streamlit]
```

---

## ğŸ”¹ Tools and Registry

Each tool/sub-agent is registered with clear schemas:

```yaml
name: gdb_query
description: "Query relationships in Neo4j AuraDB"
input_schema: { "query": "string" }
output_schema: { "nodes": "list", "relationships": "list" }
```

This ensures **consistency and interoperability**.

---

## ğŸ”¹ Embeddings and Vector Databases

- **Embedding model**: `BAAI/bge-m3`  
- **Vector store**: `Qdrant` with 3 collections:  
  - `ontology_vdb`  
  - `technical_docs_vdb`  
  - `synthetic_data_vdb` (expandable to real data).  

---

## ğŸ”¹ Graph Database

- **Neo4j AuraDB** for structured graph reasoning.  
- Entities include roles, tasks, risks, controls, incidents.  
- Queries written in **Cypher**.  

---

## ğŸ”¹ Predictive Models

- Exposed as **APIs** (FastAPI).  
- Input: structured operational data.  
- Output: incident probability, risk scores, etc.  
- Registered as `predictive_agent` tool.  

---

## ğŸ”¹ Unified Output Schema

All responses follow a unified JSON structure:

```json
{
  "answer": "Final enriched answer",
  "sources": ["ontology_vdb", "neo4j", "predictive_api"],
  "explanation": "Reasoning steps and fallback decisions",
  "metadata": {
    "retrieved_chunks": [],
    "cypher_queries": [],
    "prediction_scores": {}
  }
}
```

This guarantees **traceability and explainability**.

---

## ğŸ”¹ Evaluation and Benchmarking

We maintain a benchmark set of queries:

- **Ontology**: â€œWhat are the causes of fall-from-height incidents?â€  
- **Graph**: â€œWhich roles are connected to high-voltage tasks?â€  
- **Predictive**: â€œWhat is the probability of an incident next week in Open Pit?â€  

Evaluation includes:  
- Logging results.  
- Error analysis.  
- Accuracy and quality scoring.  

---

## ğŸ”¹ Repository Structure

```
/agents/        # Master + Sub-agents (VDB, GDB, Predictive)
/tools/         # Tool definitions with schemas
/tests/         # Benchmark queries + evaluation
/notebooks/     # Exploration and prototyping
/scripts/       # Reproducible pipelines (e.g., run_agent.py)
/frontend/      # Streamlit interface
```

---

## ğŸ”¹ Usage Example

Run the API backend:  
```bash
uvicorn scripts.run_agent:app --reload
```

Run the frontend:  
```bash
streamlit run frontend/app.py
```

---

## âœ… Best Practices

- Use **RouterQueryEngine** for lightweight routing before full agent orchestration.  
- Enable **explainability**: return retrieved chunks, Cypher queries, prediction scores.  
- Add **reflection/fallback logic** for failed tool calls.  
- Keep **tools modular** with JSON schemas.  
- Benchmark regularly with the `/tests/` module.  

---

## ğŸš€ Roadmap

- [ ] Implement modular sub-agents in LangChain.  
- [ ] Add reflection/fallback logic.  
- [ ] Standardize tool registry with JSON schemas.  
- [ ] Build reproducible pipelines (`run_agent.py`).  
- [ ] Develop evaluation module with benchmark queries.  
- [ ] Expand predictive APIs and connect real data.  
- [ ] Deploy first version of Streamlit frontend.  

---

## ğŸ”¹ Key Benefits

- **Hierarchical reasoning** â†’ interpretable, modular design.  
- **Multi-source integration** â†’ embeddings, graphs, predictions.  
- **Robustness** â†’ reflection + fallback mechanisms.  
- **Explainability** â†’ unified JSON schema for outputs.  
- **Reproducibility** â†’ pipelines and tests.  
- **Scalability** â†’ future-proof for real data and advanced models.  

---
